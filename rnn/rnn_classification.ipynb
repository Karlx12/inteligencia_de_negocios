{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a4d00d8",
      "metadata": {
        "id": "0a4d00d8"
      },
      "source": [
        "# Clasificación de Imágenes MRI con RNN\n",
        "\n",
        "Este notebook explora el uso de redes neuronales recurrentes (RNN) para la clasificación de imágenes MRI de tumores cerebrales. Incluye:\n",
        "\n",
        "1. Preprocesamiento y conversión de imágenes a secuencias\n",
        "2. Definición y entrenamiento de un modelo RNN\n",
        "3. Evaluación de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "20739fb3",
      "metadata": {
        "id": "20739fb3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Flatten\n",
        "\n",
        "# Configuración de rutas\n",
        "# DATASET_DIR = \"../total/archive/Training\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67af8312",
      "metadata": {
        "id": "67af8312"
      },
      "source": [
        "Las RNN no son ideales para imágenes, pero este notebook es educativo para explorar su uso en visión computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "PEEqwNidclqw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "PEEqwNidclqw",
        "outputId": "61829fd8-8e77-4c62-fda5-3264294b05bf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LWPaMqvrdjuA",
      "metadata": {
        "id": "LWPaMqvrdjuA"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/dataset/\")\n",
        "TRAINING_BASE_DIR = BASE_DIR / \"Training\"\n",
        "AUGMENTED_BASE_DIR = BASE_DIR / \"Augmented\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f8b77c",
      "metadata": {
        "id": "04f8b77c"
      },
      "source": [
        "# Clasificación de Tumores Cerebrales con RNN (Secuencias de Parches de Imagen)\n",
        "\n",
        "Este notebook implementa una aproximación experimental para usar RNN en imágenes MRI, convirtiendo cada imagen en una secuencia de parches y clasificando con una red recurrente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ee7372",
      "metadata": {
        "id": "e4ee7372"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization, TimeDistributed, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "classes = [d.name for d in AUGMENTED_BASE_DIR.iterdir() if d.is_dir()]\n",
        "num_classes = len(classes)\n",
        "\n",
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "DROPOUT_RATE = 0.5\n",
        "L2_REG = 0.0001\n",
        "PATCH_SIZE = 16\n",
        "PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "PATCH_DIM = PATCH_SIZE * PATCH_SIZE * 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee554ef",
      "metadata": {
        "id": "9ee554ef"
      },
      "outputs": [],
      "source": [
        "def image_to_sequence(img):\n",
        "    patches = []\n",
        "    h, w, c = img.shape\n",
        "    for i in range(0, h, PATCH_SIZE):\n",
        "        for j in range(0, w, PATCH_SIZE):\n",
        "            patch = img[i:i+PATCH_SIZE, j:j+PATCH_SIZE, :].reshape(-1)\n",
        "            patches.append(patch)\n",
        "    return np.array(patches)\n",
        "\n",
        "def generator_with_sequence(generator):\n",
        "    while True:\n",
        "        images, labels = next(generator)\n",
        "        batch_seq = np.array([image_to_sequence(img) for img in images])\n",
        "        yield batch_seq, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f26ba9",
      "metadata": {
        "id": "31f26ba9"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    str(AUGMENTED_BASE_DIR),\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    color_mode=\"rgb\",\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    str(AUGMENTED_BASE_DIR),\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=False,\n",
        "    color_mode=\"rgb\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c80508",
      "metadata": {
        "id": "98c80508"
      },
      "outputs": [],
      "source": [
        "def create_brain_tumor_rnn(input_shape=(PATCHES, PATCH_DIM), num_classes=num_classes):\n",
        "    model = Sequential([\n",
        "        SimpleRNN(128, input_shape=input_shape, return_sequences=True, kernel_regularizer=l2(L2_REG)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(DROPOUT_RATE),\n",
        "        SimpleRNN(64, return_sequences=False, kernel_regularizer=l2(L2_REG)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(DROPOUT_RATE),\n",
        "        Dense(64, activation=\"relu\", kernel_regularizer=l2(L2_REG)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(DROPOUT_RATE),\n",
        "        Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=\"categorical_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "    return model\n",
        "\n",
        "model = create_brain_tumor_rnn(input_shape=(PATCHES, PATCH_DIM), num_classes=4)\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_auc\", patience=15, mode=\"max\", restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=8),\n",
        "    ModelCheckpoint(BASE_DIR/\"models/best_rnn_model.keras\", monitor=\"val_auc\", save_best_only=True, mode=\"max\"),\n",
        "    TensorBoard(log_dir=\"./logs_rnn\"),\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    generator_with_sequence(train_generator),\n",
        "    steps_per_epoch = int(0.7 * train_generator.samples // BATCH_SIZE),\n",
        "    validation_data=generator_with_sequence(val_generator),\n",
        "    validation_steps = int(0.7 * val_generator.samples // BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")\n"
      ],
      "metadata": {
        "id": "7MMBIY2kIDgd"
      },
      "id": "7MMBIY2kIDgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc, val_auc = model.evaluate(generator_with_sequence(val_generator), steps=val_generator.samples // BATCH_SIZE)\n",
        "print(f\"\\nPrecisión Validación: {val_acc:.4f}\")\n",
        "print(f\"AUC Validación: {val_auc:.4f}\")\n",
        "\n",
        "model.save(\"brain_tumor_rnn_final.h5\")"
      ],
      "metadata": {
        "id": "4UVyu-paIAlt"
      },
      "id": "4UVyu-paIAlt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQzU4GA4IkEe"
      },
      "id": "gQzU4GA4IkEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "290997a4",
      "metadata": {
        "id": "290997a4"
      },
      "source": [
        "# Aumentación de Datos para Imágenes MRI\n",
        "\n",
        "Antes de convertir las imágenes a secuencias para la RNN, es recomendable aplicar aumentación para mejorar la robustez del modelo. A continuación se muestra cómo realizar aumentación usando `ImageDataGenerator` de Keras y visualizar ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a55b7e",
      "metadata": {
        "id": "c6a55b7e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from pathlib import Path\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "AUG_IMG_SIZE = 64\n",
        "AUG_BATCH_SIZE = 4\n",
        "\n",
        "# Crear carpetas de salida si no existen\n",
        "AUGMENTED_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "AUG_PER_IMAGE = 10  # Número de aumentaciones por imagen\n",
        "INPUT_DIR = TRAINING_BASE_DIR\n",
        "OUTPUT_DIR = AUGMENTED_BASE_DIR\n",
        "\n",
        "# Definir el pipeline de augmentación (basado en el notebook)\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Rotate(limit=30, p=0.5),\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=0.2, contrast_limit=0.2, p=0.3\n",
        "        ),\n",
        "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
        "        A.CLAHE(p=0.3),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def augment_dataset(input_dir, output_dir, aug_per_image=AUG_PER_IMAGE):\n",
        "    input_dir = Path(input_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for class_dir in input_dir.iterdir():\n",
        "        if not class_dir.is_dir():\n",
        "            continue\n",
        "        out_class_dir = output_dir / class_dir.name\n",
        "        out_class_dir.mkdir(parents=True, exist_ok=True)\n",
        "        images = sorted([f for f in class_dir.glob(\"*.jpg\")])\n",
        "        for img_path in tqdm(images, desc=f\"{class_dir.name}\"):\n",
        "            img = cv2.imread(str(img_path))\n",
        "            if img is None:\n",
        "                continue\n",
        "            # Guardar imagen original\n",
        "            orig_name = out_class_dir / img_path.name\n",
        "            cv2.imwrite(str(orig_name), img)\n",
        "            # Generar aumentaciones\n",
        "            for i in range(aug_per_image):\n",
        "                augmented = transform(image=img)\n",
        "                aug_img = augmented[\"image\"]\n",
        "                aug_name = (\n",
        "                    out_class_dir / f\"{img_path.stem}_aug{i}{img_path.suffix}\"\n",
        "                )\n",
        "                cv2.imwrite(str(aug_name), aug_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  augment_dataset(INPUT_DIR, OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "aHDqlgeJjV4O"
      },
      "id": "aHDqlgeJjV4O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -p  /content/Augmented/glioma/ | grep -v / | wc -l"
      ],
      "metadata": {
        "id": "jyWHenmXkwcu"
      },
      "id": "jyWHenmXkwcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TR4g2LoZ_1zw"
      },
      "id": "TR4g2LoZ_1zw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}